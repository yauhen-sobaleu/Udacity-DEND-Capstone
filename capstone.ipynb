{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datalake for USA Wildfires\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project focuses on building a datalake on AWS S3 with USA wildfires and USA weather outliers data using star-schema data modelling technique for both ad-hoc analyses and batch processing.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.28.0/sqlite-jdbc-3.28.0.jar\n",
      "Finished download of sqlite-jdbc-3.28.0.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.28.0/sqlite-jdbc-3.28.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import java.time.{ZonedDateTime, LocalDateTime, LocalDate}\n",
    "import java.time.format.DateTimeFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "During this project we will read in an sqlite database with USA wildfires data together with a temperature outliers dataset using Apache Spark and store resulting data in a datalake on s3. The data model of the project is to create a star-schema pattern with resulting tables and store it on s3 in corresponding buckets with Amazon Athena on top for ad-hoc querying. Resulting data can be used to analyze geo-spatial information from both datasets to find correlation between unusual weather conditions and the probability of occuring a wildfire by superimposing coordinates data from both datasets or using K-means algorythm. Also it can be used for measuring the efficiency of containing wildfires by responsible agencies.\n",
    "\n",
    "We will use Scala API of Apache Spark for this project with an sbt-assembly \"fat\" jar as a final deliverable. Choice of language is dictated by perfomance benefits of using Scala as a JVM language that is native to Spark.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "We will use:\n",
    "* [1.88 Million US Wildfires dataset](https://www.kaggle.com/rtatman/188-million-us-wildfires) by Rachael Tatman\n",
    "\n",
    "Dataset with a spatial database of wildfires that occurred in the United States from 1992 to 2015. The wildfire records were acquired from the reporting systems of federal, state, and local fire organizations.\n",
    "\n",
    "* [U.S. Weather Outliers](https://data.world/carlvlewis/u-s-weather-outliers-1964) by Berkeley Earth\n",
    "\n",
    "U.S. Weather Outliers data from 19964 to 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USA Wildfires Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [OBJECTID: int, FOD_ID: decimal(38,18) ... 36 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[OBJECTID: int, FOD_ID: decimal(38,18) ... 36 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"jdbc\")\n",
    "      .option(\"url\", \"jdbc:sqlite:/Users/yauhensobaleu/Downloads/FPA_FOD_20170508.sqlite\")\n",
    "      .option(\"dbtable\",\n",
    "        \"\"\"\n",
    "          |(select\n",
    "          |   OBJECTID,\n",
    "          |   FOD_ID,\n",
    "          |   FPA_ID,\n",
    "          |   SOURCE_SYSTEM_TYPE,\n",
    "          |   SOURCE_SYSTEM,\n",
    "          |   NWCG_REPORTING_AGENCY,\n",
    "          |   NWCG_REPORTING_UNIT_ID,\n",
    "          |   NWCG_REPORTING_UNIT_NAME,\n",
    "          |   SOURCE_REPORTING_UNIT,\n",
    "          |   SOURCE_REPORTING_UNIT_NAME,\n",
    "          |   LOCAL_FIRE_REPORT_ID,\n",
    "          |   LOCAL_INCIDENT_ID,\n",
    "          |   FIRE_CODE,\n",
    "          |   FIRE_NAME,\n",
    "          |   ICS_209_INCIDENT_NUMBER,\n",
    "          |   ICS_209_NAME,\n",
    "          |   MTBS_ID,\n",
    "          |   MTBS_FIRE_NAME,\n",
    "          |   COMPLEX_NAME,\n",
    "          |   FIRE_YEAR,\n",
    "          |   DISCOVERY_DATE,\n",
    "          |   DISCOVERY_DOY,\n",
    "          |   DISCOVERY_TIME,\n",
    "          |   STAT_CAUSE_CODE,\n",
    "          |   STAT_CAUSE_DESCR,\n",
    "          |   CONT_DATE,\n",
    "          |   CONT_DOY,\n",
    "          |   CONT_TIME,\n",
    "          |   FIRE_SIZE,\n",
    "          |   FIRE_SIZE_CLASS,\n",
    "          |   LATITUDE,\n",
    "          |   LONGITUDE,\n",
    "          |   OWNER_CODE,\n",
    "          |   OWNER_DESCR,\n",
    "          |   STATE,\n",
    "          |   COUNTY,\n",
    "          |   FIPS_CODE,\n",
    "          |   FIPS_NAME\n",
    "          |FROM Fires)\"\"\".stripMargin)\n",
    "      .option(\"driver\", \"org.sqlite.JDBC\")\n",
    "      .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1880465"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the schema of wildfires dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OBJECTID: integer (nullable = true)\n",
      " |-- FOD_ID: decimal(38,18) (nullable = true)\n",
      " |-- FPA_ID: string (nullable = true)\n",
      " |-- SOURCE_SYSTEM_TYPE: string (nullable = true)\n",
      " |-- SOURCE_SYSTEM: string (nullable = true)\n",
      " |-- NWCG_REPORTING_AGENCY: string (nullable = true)\n",
      " |-- NWCG_REPORTING_UNIT_ID: string (nullable = true)\n",
      " |-- NWCG_REPORTING_UNIT_NAME: string (nullable = true)\n",
      " |-- SOURCE_REPORTING_UNIT: string (nullable = true)\n",
      " |-- SOURCE_REPORTING_UNIT_NAME: string (nullable = true)\n",
      " |-- LOCAL_FIRE_REPORT_ID: string (nullable = true)\n",
      " |-- LOCAL_INCIDENT_ID: string (nullable = true)\n",
      " |-- FIRE_CODE: string (nullable = true)\n",
      " |-- FIRE_NAME: string (nullable = true)\n",
      " |-- ICS_209_INCIDENT_NUMBER: string (nullable = true)\n",
      " |-- ICS_209_NAME: string (nullable = true)\n",
      " |-- MTBS_ID: string (nullable = true)\n",
      " |-- MTBS_FIRE_NAME: string (nullable = true)\n",
      " |-- COMPLEX_NAME: string (nullable = true)\n",
      " |-- FIRE_YEAR: decimal(38,18) (nullable = true)\n",
      " |-- DISCOVERY_DATE: decimal(38,18) (nullable = true)\n",
      " |-- DISCOVERY_DOY: decimal(38,18) (nullable = true)\n",
      " |-- DISCOVERY_TIME: string (nullable = true)\n",
      " |-- STAT_CAUSE_CODE: decimal(38,18) (nullable = true)\n",
      " |-- STAT_CAUSE_DESCR: string (nullable = true)\n",
      " |-- CONT_DATE: decimal(38,18) (nullable = true)\n",
      " |-- CONT_DOY: decimal(38,18) (nullable = true)\n",
      " |-- CONT_TIME: string (nullable = true)\n",
      " |-- FIRE_SIZE: decimal(38,18) (nullable = true)\n",
      " |-- FIRE_SIZE_CLASS: string (nullable = true)\n",
      " |-- LATITUDE: decimal(38,18) (nullable = true)\n",
      " |-- LONGITUDE: decimal(38,18) (nullable = true)\n",
      " |-- OWNER_CODE: decimal(38,18) (nullable = true)\n",
      " |-- OWNER_DESCR: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- COUNTY: string (nullable = true)\n",
      " |-- FIPS_CODE: string (nullable = true)\n",
      " |-- FIPS_NAME: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issue 1. Decimal precision is too big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some columns with `Decimal(38,18)` data type which we can convert to more representable and less memory consuming format. Let's make a list of all Decimal columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decimalColumns = Array(FOD_ID, FIRE_YEAR, DISCOVERY_DATE, DISCOVERY_DOY, STAT_CAUSE_CODE, CONT_DATE, CONT_DOY, FIRE_SIZE, LATITUDE, LONGITUDE, OWNER_CODE)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[FOD_ID, FIRE_YEAR, DISCOVERY_DATE, DISCOVERY_DOY, STAT_CAUSE_CODE, CONT_DATE, CONT_DOY, FIRE_SIZE, LATITUDE, LONGITUDE, OWNER_CODE]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val decimalColumns = for (dtype <- df.dtypes if (dtype._2.startsWith(\"DecimalType\")))\n",
    "  yield dtype._1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>FOD_ID</th><th>FIRE_YEAR</th><th>DISCOVERY_DATE</th><th>DISCOVERY_DOY</th><th>STAT_CAUSE_CODE</th><th>CONT_DATE</th><th>CONT_DOY</th><th>FIRE_SIZE</th><th>LATITUDE</th><th>LONGITUDE</th><th>OWNER_CODE</th></tr><tr><td>1.000000000000000000</td><td>2005.000000000000000000</td><td>2453403.500000000000000000</td><td>33.000000000000000000</td><td>9.000000000000000000</td><td>2453403.500000000000000000</td><td>33.000000000000000000</td><td>0.100000000000000000</td><td>40.036944440000000000</td><td>-121.005833330000000000</td><td>5.000000000000000000</td></tr><tr><td>2.000000000000000000</td><td>2004.000000000000000000</td><td>2453137.500000000000000000</td><td>133.000000000000000000</td><td>1.000000000000000000</td><td>2453137.500000000000000000</td><td>133.000000000000000000</td><td>0.250000000000000000</td><td>38.933055560000000000</td><td>-120.404444440000000000</td><td>5.000000000000000000</td></tr></table>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 2\n",
    "df.select(decimalColumns.map(name => col(name)):_*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert integer-valued columns to `IntegerType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfWithIntegers = [OBJECTID: int, FOD_ID: int ... 36 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[OBJECTID: int, FOD_ID: int ... 36 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithIntegers = df\n",
    "    .withColumn(\"FOD_ID\", $\"FOD_ID\".cast(\"int\"))\n",
    "    .withColumn(\"FIRE_YEAR\", $\"FIRE_YEAR\".cast(\"int\"))\n",
    "    .withColumn(\"DISCOVERY_DOY\", $\"DISCOVERY_DOY\".cast(\"int\"))\n",
    "    .withColumn(\"STAT_CAUSE_CODE\", $\"STAT_CAUSE_CODE\".cast(\"int\"))\n",
    "    .withColumn(\"CONT_DOY\", $\"CONT_DOY\".cast(\"int\"))\n",
    "    .withColumn(\"OWNER_CODE\", $\"OWNER_CODE\".cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>FOD_ID</th><th>FIRE_YEAR</th><th>DISCOVERY_DATE</th><th>DISCOVERY_DOY</th><th>STAT_CAUSE_CODE</th><th>CONT_DATE</th><th>CONT_DOY</th><th>FIRE_SIZE</th><th>LATITUDE</th><th>LONGITUDE</th><th>OWNER_CODE</th></tr><tr><td>1</td><td>2005</td><td>2453403.500000000000000000</td><td>33</td><td>9</td><td>2453403.500000000000000000</td><td>33</td><td>0.100000000000000000</td><td>40.036944440000000000</td><td>-121.005833330000000000</td><td>5</td></tr><tr><td>2</td><td>2004</td><td>2453137.500000000000000000</td><td>133</td><td>1</td><td>2453137.500000000000000000</td><td>133</td><td>0.250000000000000000</td><td>38.933055560000000000</td><td>-120.404444440000000000</td><td>5</td></tr></table>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 2\n",
    "dfWithIntegers.select(decimalColumns.map(name => col(name)):_*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are columns that do have floating point value. Let's find maximum values for these columns to understand to what extent can we reduce the size of fixed and precision values in decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------------------------+-------------------------+\n",
      "|max(DISCOVERY_DATE)       |max(CONT_DATE)            |max(FIRE_SIZE)           |\n",
      "+--------------------------+--------------------------+-------------------------+\n",
      "|2457387.500000000000000000|2457391.500000000000000000|606945.000000000000000000|\n",
      "+--------------------------+--------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithIntegers.agg(\"DISCOVERY_DATE\" -> \"max\", \"CONT_DATE\" -> \"max\", \"FIRE_SIZE\" -> \"max\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be reasonable to decrease both the size of fixed and precision values in these columns to (10,2) except coordinates columns that have \"-\" sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "properDecimalSize = decimal(10,2)\n",
       "coordinatesDecimalSize = decimal(11,8)\n",
       "dfWithIntegersProperDecimals = [OBJECTID: int, FOD_ID: int ... 36 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[OBJECTID: int, FOD_ID: int ... 36 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val properDecimalSize = \"decimal(10,2)\"\n",
    "val coordinatesDecimalSize = \"decimal(11,8)\"\n",
    "\n",
    "val dfWithIntegersProperDecimals = dfWithIntegers\n",
    "    .withColumn(\"DISCOVERY_DATE\", $\"DISCOVERY_DATE\".cast(properDecimalSize))\n",
    "    .withColumn(\"CONT_DATE\", $\"CONT_DATE\".cast(properDecimalSize))\n",
    "    .withColumn(\"FIRE_SIZE\", $\"FIRE_SIZE\".cast(properDecimalSize))\n",
    "    .withColumn(\"LATITUDE\", $\"LATITUDE\".cast(coordinatesDecimalSize))\n",
    "    .withColumn(\"LONGITUDE\", $\"LONGITUDE\".cast(coordinatesDecimalSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it looks more friendly to an end user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>OBJECTID</th><th>FOD_ID</th><th>FPA_ID</th><th>SOURCE_SYSTEM_TYPE</th><th>SOURCE_SYSTEM</th><th>NWCG_REPORTING_AGENCY</th><th>NWCG_REPORTING_UNIT_ID</th><th>NWCG_REPORTING_UNIT_NAME</th><th>SOURCE_REPORTING_UNIT</th><th>SOURCE_REPORTING_UNIT_NAME</th><th>LOCAL_FIRE_REPORT_ID</th><th>LOCAL_INCIDENT_ID</th><th>FIRE_CODE</th><th>FIRE_NAME</th><th>ICS_209_INCIDENT_NUMBER</th><th>ICS_209_NAME</th><th>MTBS_ID</th><th>MTBS_FIRE_NAME</th><th>COMPLEX_NAME</th><th>FIRE_YEAR</th><th>DISCOVERY_DATE</th><th>DISCOVERY_DOY</th><th>DISCOVERY_TIME</th><th>STAT_CAUSE_CODE</th><th>STAT_CAUSE_DESCR</th><th>CONT_DATE</th><th>CONT_DOY</th><th>CONT_TIME</th><th>FIRE_SIZE</th><th>FIRE_SIZE_CLASS</th><th>LATITUDE</th><th>LONGITUDE</th><th>OWNER_CODE</th><th>OWNER_DESCR</th><th>STATE</th><th>COUNTY</th><th>FIPS_CODE</th><th>FIPS_NAME</th></tr><tr><td>1</td><td>1</td><td>FS-1418826</td><td>FED</td><td>FS-FIRESTAT</td><td>FS</td><td>USCAPNF</td><td>Plumas National Forest</td><td>0511</td><td>Plumas National Forest</td><td>1</td><td>PNF-47</td><td>BJ8K</td><td>FOUNTAIN</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2005</td><td>2453403.50</td><td>33</td><td>1300</td><td>9</td><td>Miscellaneous</td><td>2453403.50</td><td>33</td><td>1730</td><td>0.10</td><td>A</td><td>40.03694444</td><td>-121.00583333</td><td>5</td><td>USFS</td><td>CA</td><td>63</td><td>063</td><td>Plumas</td></tr><tr><td>2</td><td>2</td><td>FS-1418827</td><td>FED</td><td>FS-FIRESTAT</td><td>FS</td><td>USCAENF</td><td>Eldorado National Forest</td><td>0503</td><td>Eldorado National Forest</td><td>13</td><td>13</td><td>AAC0</td><td>PIGEON</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2004</td><td>2453137.50</td><td>133</td><td>0845</td><td>1</td><td>Lightning</td><td>2453137.50</td><td>133</td><td>1530</td><td>0.25</td><td>A</td><td>38.93305556</td><td>-120.40444444</td><td>5</td><td>USFS</td><td>CA</td><td>61</td><td>061</td><td>Placer</td></tr></table>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 2\n",
    "dfWithIntegersProperDecimals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issue 2. Null values\n",
    "Let's make a quick look at the number of NULL values in every column of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nullCountsPerColumn = Array((OBJECTID,0), (FOD_ID,0), (FPA_ID,0), (SOURCE_SYSTEM_TYPE,0), (SOURCE_SYSTEM,0), (NWCG_REPORTING_AGENCY,0), (NWCG_REPORTING_UNIT_ID,0), (NWCG_REPORTING_UNIT_NAME,0), (SOURCE_REPORTING_UNIT,0), (SOURCE_REPORTING_UNIT_NAME,0), (LOCAL_FIRE_REPORT_ID,1459286), (LOCAL_INCIDENT_ID,820821), (FIRE_CODE,1555636), (FIRE_NAME,957189), (ICS_209_INCIDENT_NUMBER,1854748), (ICS_209_NAME,1854748), (MTBS_ID,1869462), (MTBS_FIRE_NAME,1869462), (COMPLEX_NAME,1875282), (FIRE_YEAR,0), (DISCOVERY_DATE,0), (DISCOVERY_DOY,0), (DISCOVERY_TIME,882638), (STAT_CAUSE_CODE,0), (STAT_CAUSE_DESCR,0), (CONT_DATE,891531), (CONT_DOY,891531), (CONT_TIME,972173), (FIRE_SIZE,0), (FIRE_SIZE_CLASS,0), (LATITUDE,0), (LONGITUDE,0), (OWNER_CODE,0), (OWNER_DESCR,0), (STATE,0), (C...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(OBJECTID,0), (FOD_ID,0), (FPA_ID,0), (SOURCE_SYSTEM_TYPE,0), (SOURCE_SYSTEM,0), (NWCG_REPORTING_AGENCY,0), (NWCG_REPORTING_UNIT_ID,0), (NWCG_REPORTING_UNIT_NAME,0), (SOURCE_REPORTING_UNIT,0), (SOURCE_REPORTING_UNIT_NAME,0), (LOCAL_FIRE_REPORT_ID,1459286), (LOCAL_INCIDENT_ID,820821), (FIRE_CODE,1555636), (FIRE_NAME,957189), (ICS_209_INCIDENT_NUMBER,1854748), (ICS_209_NAME,1854748), (MTBS_ID,1869462), (MTBS_FIRE_NAME,1869462), (COMPLEX_NAME,1875282), (FIRE_YEAR,0), (DISCOVERY_DATE,0), (DISCOVERY_DOY,0), (DISCOVERY_TIME,882638), (STAT_CAUSE_CODE,0), (STAT_CAUSE_DESCR,0), (CONT_DATE,891531), (CONT_DOY,891531), (CONT_TIME,972173), (FIRE_SIZE,0), (FIRE_SIZE_CLASS,0), (LATITUDE,0), (LONGITUDE,0), (OWNER_CODE,0), (OWNER_DESCR,0), (STATE,0), (COUNTY,678148), (FIPS_CODE,678148), (FIPS_NAME,678148)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nullCountsPerColumn = dfWithIntegersProperDecimals.columns.map(\n",
    "    c => (c, dfWithIntegersProperDecimals.agg(\n",
    "        sum(when(dfWithIntegersProperDecimals(c).isNull, 1).otherwise(0)).alias(c)\n",
    "    ).first().getLong(0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>column</th><th>countna</th></tr><tr><td>OBJECTID</td><td>0</td></tr><tr><td>FOD_ID</td><td>0</td></tr><tr><td>FPA_ID</td><td>0</td></tr><tr><td>SOURCE_SYSTEM_TYPE</td><td>0</td></tr><tr><td>SOURCE_SYSTEM</td><td>0</td></tr><tr><td>NWCG_REPORTING_AGENCY</td><td>0</td></tr><tr><td>NWCG_REPORTING_UNIT_ID</td><td>0</td></tr><tr><td>NWCG_REPORTING_UNIT_NAME</td><td>0</td></tr><tr><td>SOURCE_REPORTING_UNIT</td><td>0</td></tr><tr><td>SOURCE_REPORTING_UNIT_NAME</td><td>0</td></tr><tr><td>LOCAL_FIRE_REPORT_ID</td><td>1459286</td></tr><tr><td>LOCAL_INCIDENT_ID</td><td>820821</td></tr><tr><td>FIRE_CODE</td><td>1555636</td></tr><tr><td>FIRE_NAME</td><td>957189</td></tr><tr><td>ICS_209_INCIDENT_NUMBER</td><td>1854748</td></tr><tr><td>ICS_209_NAME</td><td>1854748</td></tr><tr><td>MTBS_ID</td><td>1869462</td></tr><tr><td>MTBS_FIRE_NAME</td><td>1869462</td></tr><tr><td>COMPLEX_NAME</td><td>1875282</td></tr><tr><td>FIRE_YEAR</td><td>0</td></tr><tr><td>DISCOVERY_DATE</td><td>0</td></tr><tr><td>DISCOVERY_DOY</td><td>0</td></tr><tr><td>DISCOVERY_TIME</td><td>882638</td></tr><tr><td>STAT_CAUSE_CODE</td><td>0</td></tr><tr><td>STAT_CAUSE_DESCR</td><td>0</td></tr><tr><td>CONT_DATE</td><td>891531</td></tr><tr><td>CONT_DOY</td><td>891531</td></tr><tr><td>CONT_TIME</td><td>972173</td></tr><tr><td>FIRE_SIZE</td><td>0</td></tr><tr><td>FIRE_SIZE_CLASS</td><td>0</td></tr><tr><td>LATITUDE</td><td>0</td></tr><tr><td>LONGITUDE</td><td>0</td></tr><tr><td>OWNER_CODE</td><td>0</td></tr><tr><td>OWNER_DESCR</td><td>0</td></tr><tr><td>STATE</td><td>0</td></tr><tr><td>COUNTY</td><td>678148</td></tr><tr><td>FIPS_CODE</td><td>678148</td></tr><tr><td>FIPS_NAME</td><td>678148</td></tr></table>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 50\n",
    "sc.parallelize(nullCountsPerColumn).toDF(\"column\", \"countna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several columns have quite a lot of null values. The most imporant thing is that every wildfire record has its `DISCOVERY_DATE` specified. We can rely on 100% coverage of this column for any downstream purposes (like partitioning the resulting parquet file by discovery date and not by the date of containing a wildfire as it has a big chunk of nulls). Also it's worth mentioning that we can't specify the exact time of occuring a wildfire as `DISCOVERY_TIME` column has nulls as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issue 3. Multiple date columns in Julian format\n",
    "Columns `CONT_DATE` and `DISCOVERY_DATE` have Julian date format. Also the dataset has separate `*_DAY` and `*_TIME` columns. We can combine them into one timestamp column being mindful that `*_TIME` columns can have null values. \n",
    "\n",
    "Also let's discard `*_DOY` columns as this data will be present in date dimension dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly let's fill null values in `*_TIME` columns with `0000` to be able to concatenate it with date values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfFilled = [OBJECTID: int, FOD_ID: int ... 36 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<console>:6: error: Symbol 'type scala.AnyRef' is missing from the classpath.\n",
       "This symbol is required by 'class org.apache.spark.sql.types.MetadataBuilder'.\n",
       "Make sure that type AnyRef is in your classpath and check for conflicting dependencies with `-Ylog-classpath`.\n",
       "A full rebuild may help if 'MetadataBuilder.class' was compiled against an incompatible version of scala.\n",
       "  lazy val $print: String =  {\n",
       "           ^\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[OBJECTID: int, FOD_ID: int ... 36 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfFilled = dfWithIntegersProperDecimals.na.fill(\"0000\", Seq(\"CONT_TIME\", \"DISCOVERY_TIME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a udf function that converts Julian date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convertJulianDate: org.apache.spark.sql.expressions.UserDefinedFunction\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convertJulianDate = udf((julianDate: Double) => {\n",
    "  val julianDayNumber = math.floor(julianDate).toLong\n",
    "  val julianDayFraction = julianDate - julianDayNumber\n",
    "  val julianDayFractionToNanoSeconds = math.floor(julianDayFraction * 24 * 60 * 60 * math.pow(10, 9)).toLong\n",
    "\n",
    "  val bcEraDateFormat = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss:z:G\")\n",
    "  val julianDateStartDate = ZonedDateTime.parse(\"4714-11-24 12:00:00:GMT:BC\", bcEraDateFormat)\n",
    "\n",
    "  val zonedDateTime = julianDateStartDate.plusDays(julianDayNumber).plusNanos(julianDayFractionToNanoSeconds)\n",
    "\n",
    "  zonedDateTime.toLocalDate.toString()\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom function for concatenating date and time in wildfires dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "concatDateAndTime: (df: org.apache.spark.sql.DataFrame, colName: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def concatDateAndTime(df:DataFrame, colName: String): DataFrame = {\n",
    "    \n",
    "    val dateColumn = colName + \"_DATE\"\n",
    "    val timeColumn = colName + \"_TIME\"\n",
    "    \n",
    "    val resultDF = df.withColumn(colName + \"_TIMESTAMP\", \n",
    "        concat(\n",
    "            convertJulianDate(col(dateColumn)), lit(\" \"), regexp_replace(col(timeColumn), \"..(?!$)\", \"$0:\"), lit(\":00\")\n",
    "    ).cast(\"timestamp\"))\n",
    "    \n",
    "    resultDF\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datetime columns and drop redundant `*_DOY`,  `*_TIME` and `FIRE_YEAR` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfWithTimestampsDiscovery = [OBJECTID: int, FOD_ID: int ... 37 more fields]\n",
       "dfWithTimestampsCont = [OBJECTID: int, FOD_ID: int ... 38 more fields]\n",
       "stagedDF = [objectid: int, fod_id: int ... 32 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[objectid: int, fod_id: int ... 32 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithTimestampsDiscovery = concatDateAndTime(dfFilled, \"DISCOVERY\")\n",
    "val dfWithTimestampsCont = concatDateAndTime(dfWithTimestampsDiscovery, \"CONT\")\n",
    "\n",
    "val stagedDF = dfWithTimestampsCont\n",
    "    .toDF(dfWithTimestampsCont.columns.map(name => name.toLowerCase()):_*)\n",
    "    .drop( \n",
    "    \"discovery_doy\", \n",
    "    \"discovery_date\",\n",
    "    \"discovery_time\",\n",
    "    \"cont_doy\",\n",
    "    \"cont_date\",\n",
    "    \"cont_time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temperatures = [date_str: string, degrees_from_mean: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[date_str: string, degrees_from_mean: string ... 6 more fields]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val temperatures = spark.read.format(\"com.databricks.spark.csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"/Users/yauhensobaleu/Downloads/weather-anomalies-1964-2013.csv\")\n",
    "    .drop(\"id\", \"station_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in temperature data starting from 1992-01-01 only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stagedWeatherDF = [date_str: date, degrees_from_mean: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[date_str: date, degrees_from_mean: string ... 6 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stagedWeatherDF = temperatures\n",
    "    .withColumn(\"date_str\", $\"date_str\".cast(\"date\"))\n",
    "    .filter($\"date_str\" >= \"1992-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1352248"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stagedWeatherDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>date_str</th><th>degrees_from_mean</th><th>longitude</th><th>latitude</th><th>max_temp</th><th>min_temp</th><th>type</th><th>serialid</th></tr><tr><td>2008-11-02</td><td>15.14</td><td>-99.0383</td><td>45.4478</td><td>20.0</td><td>0.0</td><td>Weak Hot</td><td>287175</td></tr><tr><td>2002-06-19</td><td>7.02</td><td>-106.4278</td><td>36.2403</td><td>35.6</td><td>13.3</td><td>Weak Hot</td><td>300563</td></tr><tr><td>2002-06-19</td><td>9.32</td><td>-84.9767</td><td>45.3725</td><td>31.1</td><td>20.6</td><td>Weak Hot</td><td>300564</td></tr><tr><td>2002-06-19</td><td>14.86</td><td>-117.9528</td><td>36.1389</td><td>40.6</td><td>21.7</td><td>Strong Hot</td><td>300565</td></tr><tr><td>2002-06-19</td><td>9.52</td><td>-110.395</td><td>40.1678</td><td>29.4</td><td>18.3</td><td>Weak Hot</td><td>300566</td></tr></table>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 5\n",
    "stagedWeatherDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "Project's data model consists of two fact tables `fires_fact` and `weather_fact` with appropriated SCD type 1 dimension tables respresenting data from USA wildfires and USA Weather Outliers datasets respectively. In terms of phisycal representation every table in our schema is a bucket with arbitrary number of parquet files on AWS S3.\n",
    "\n",
    "Data model of the project:\n",
    "![Data Model](https://i.imgur.com/oTnb9x1.jpg)\n",
    "\n",
    "Dimensions of `fires_fact` table:\n",
    "* `sources` - tracks down a record to any original source that usa_wildfires was made from\n",
    "* `reporter` - lists agency names that reported a wild fire\n",
    "* `fire_names` - names of wild fires (if exist)\n",
    "* `fire_cause` - causes of fires and its codes\n",
    "* `fire_classes` - codes for fire size based on the number of acres within the final fire perimeter expenditures\n",
    "* `owners` - codes for primary owners or entities responsible for managing the land at the point of origin of the fire\n",
    "* `locations` - locations of a wildfire\n",
    "\n",
    "Dimensions of `weather_fact` table:\n",
    "* `weather_types` - types of weather (e.g. Very Cold)\n",
    "\n",
    "Common dimension:\n",
    "* `dates` - dates dimension\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1. Create a spark job that will:\n",
    "    1. Read in all datasets in spark dataframes\n",
    "    2. Clean and conform data in these datasets\n",
    "    2. Create dimension tables \n",
    "    3. Populate fact tables joining together foreign keys of all related dimension tables\n",
    "    4. Store results on AWS S3\n",
    "2. Package spark application in a \"fat\" jar with custom libraries\n",
    "3. Launch spark-submit to execute spark application\n",
    "4. Create Amazon Athena tables on top of AWS S3 buckets and perform data quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that creates a dimension dataset with `SELECT DISTINCT` logic and serial column to uniquely identify each record.\n",
    "\n",
    "NB: It's important to coalesce the results of a query into one partition to guarantee the consecutiveness of values generated by `monotonically_increasing_id()` function (see [docs](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.monotonically_increasing_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createDimTable: (df: org.apache.spark.sql.DataFrame, id: String, cols: Seq[String])org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def createDimTable(df: DataFrame, id: String, cols: Seq[String]): DataFrame = {\n",
    "    val raw_col_names = cols.map(name => name.toLowerCase())\n",
    "    val dim_id = id.toLowerCase()\n",
    "    val nullPlaceholder = \"unknown\" \n",
    "    \n",
    "    val rawDimTable = df\n",
    "        .select(raw_col_names.map(name => col(name)):_*)\n",
    "//         .na.fill(nullPlaceholder) /* replace all nulls with a string placeholder for easier joins */\n",
    "        .distinct \n",
    "        .coalesce(1) /* coalesce df in one partition to guarantee that monotonically_increasing_id() provide consecutive result */\n",
    "        .orderBy(raw_col_names(0))\n",
    "        .withColumn(dim_id, monotonically_increasing_id() + 1)\n",
    "    \n",
    "    val col_names = Seq(dim_id) ++ raw_col_names\n",
    "    val dimTable = rawDimTable.select(col_names.map(name => col(name)):_*)\n",
    "    \n",
    "    dimTable\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sourceCols = List(source_system_type, source_system, source_reporting_unit, source_reporting_unit_name)\n",
       "sources = [source_id: bigint, source_system_type: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[source_id: bigint, source_system_type: string ... 3 more fields]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sourceCols = Seq(\n",
    "    \"source_system_type\", \n",
    "    \"source_system\", \n",
    "    \"source_reporting_unit\", \n",
    "    \"source_reporting_unit_name\")\n",
    "\n",
    "val sources = createDimTable(stagedDF, \"source_id\", sourceCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>source_id</th><th>source_system_type</th><th>source_system</th><th>source_reporting_unit</th><th>source_reporting_unit_name</th></tr><tr><td>1</td><td>FED</td><td>FS-FIRESTAT</td><td>0507</td><td>Los Padres National Forest</td></tr><tr><td>2</td><td>FED</td><td>FS-FIRESTAT</td><td>0412</td><td>Payette National Forest</td></tr><tr><td>3</td><td>FED</td><td>DOI-WFMI</td><td>MTRBA</td><td>Rocky Boy's Agency</td></tr><tr><td>4</td><td>FED</td><td>DOI-WFMI</td><td>IDBUD</td><td>Burley Field Office</td></tr><tr><td>5</td><td>FED</td><td>DOI-WFMI</td><td>ARHOP</td><td>Hot Springs National Park</td></tr></table>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 5\n",
    "sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reports dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reportsCols = List(nwcg_reporting_agency, nwcg_reporting_unit_id, nwcg_reporting_unit_name)\n",
       "reports = [reporter_id: bigint, nwcg_reporting_agency: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[reporter_id: bigint, nwcg_reporting_agency: string ... 2 more fields]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reportsCols = Seq(\n",
    "    \"nwcg_reporting_agency\", \n",
    "    \"nwcg_reporting_unit_id\", \n",
    "    \"nwcg_reporting_unit_name\"\n",
    "    )\n",
    "\n",
    "val reports = createDimTable(stagedDF, \"reporter_id\", reportsCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>reporter_id</th><th>nwcg_reporting_agency</th><th>nwcg_reporting_unit_id</th><th>nwcg_reporting_unit_name</th></tr><tr><td>1</td><td>BIA</td><td>USNVWNA</td><td>Western Nevada Agency</td></tr><tr><td>2</td><td>BIA</td><td>USMTCRA</td><td>Crow Agency</td></tr><tr><td>3</td><td>BIA</td><td>USAZSCA</td><td>San Carlos Agency</td></tr><tr><td>4</td><td>BIA</td><td>USMNRLA</td><td>Red Lake Agency</td></tr><tr><td>5</td><td>BIA</td><td>USMTNCA</td><td>Northern Cheyenne Agency</td></tr></table>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 5\n",
    "reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire_names dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fireNamesCols = List(fire_name, ics_209_name, fire_code, mtbs_fire_name)\n",
       "fireNames = [fire_name_id: bigint, fire_name: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[fire_name_id: bigint, fire_name: string ... 3 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fireNamesCols = Seq(\n",
    "    \"fire_name\",\n",
    "    \"ics_209_name\",\n",
    "    \"fire_code\", \n",
    "    \"mtbs_fire_name\")\n",
    "\n",
    "val fireNames = createDimTable(stagedDF, \"fire_name_id\", fireNamesCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>fire_name_id</th><th>fire_name</th><th>ics_209_name</th><th>fire_code</th><th>mtbs_fire_name</th></tr><tr><td>1</td><td> 0703</td><td>unknown</td><td>JAQ9</td><td>unknown</td></tr><tr><td>2</td><td> 1789 CO RD 21 N, AL</td><td>unknown</td><td>unknown</td><td>unknown</td></tr><tr><td>3</td><td> 201 SELAWIK RIVER #4</td><td>unknown</td><td>GXH6</td><td>unknown</td></tr><tr><td>4</td><td> 2478 CENTER CHURCH RD, AL</td><td>unknown</td><td>unknown</td><td>unknown</td></tr><tr><td>5</td><td> 3000 HOMESTEAD, AL</td><td>unknown</td><td>unknown</td><td>unknown</td></tr></table>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 5\n",
    "fireNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire cause dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fireCausesCols = List(stat_cause_code, stat_cause_descr)\n",
       "fireCauses = [fire_cause_id: bigint, stat_cause_code: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[fire_cause_id: bigint, stat_cause_code: int ... 1 more field]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fireCausesCols = Seq(\n",
    "    \"stat_cause_code\",\n",
    "    \"stat_cause_descr\")\n",
    "\n",
    "val fireCauses = createDimTable(stagedDF, \"fire_cause_id\", fireCausesCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>fire_cause_id</th><th>stat_cause_code</th><th>stat_cause_descr</th></tr><tr><td>1</td><td>1</td><td>Lightning</td></tr><tr><td>2</td><td>2</td><td>Equipment Use</td></tr><tr><td>3</td><td>3</td><td>Smoking</td></tr><tr><td>4</td><td>4</td><td>Campfire</td></tr><tr><td>5</td><td>5</td><td>Debris Burning</td></tr></table>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 5\n",
    "fireCauses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire sizes dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fireSizesCols = List(fire_size_class)\n",
       "fireSizes = [fire_size_id: bigint, fire_size_class: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[fire_size_id: bigint, fire_size_class: string ... 2 more fields]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fireSizesCols = Seq(\n",
    "    \"fire_size_class\")\n",
    "\n",
    "val fireSizes = createDimTable(stagedDF, \"fire_size_id\", fireSizesCols)\n",
    "        .withColumn(\"lower_bound\", \n",
    "                when($\"fire_size_class\" === \"A\", 0)\n",
    "                when($\"fire_size_class\" === \"B\", 0.26)\n",
    "                when($\"fire_size_class\" === \"C\", 10.0)\n",
    "                when($\"fire_size_class\" === \"D\", 100)\n",
    "                when($\"fire_size_class\" === \"E\", 300)\n",
    "                when($\"fire_size_class\" === \"F\", 1000)\n",
    "                when($\"fire_size_class\" === \"G\", 5000)\n",
    "                   )\n",
    "        .withColumn(\"upper_bound\", \n",
    "                when($\"fire_size_class\" === \"A\", 0.25)\n",
    "                when($\"fire_size_class\" === \"B\", 9.9)\n",
    "                when($\"fire_size_class\" === \"C\", 99.9)\n",
    "                when($\"fire_size_class\" === \"D\", 299)\n",
    "                when($\"fire_size_class\" === \"E\", 999)\n",
    "                when($\"fire_size_class\" === \"F\", 4999)\n",
    "                when($\"fire_size_class\" === \"G\", null)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>fire_size_id</th><th>fire_size_class</th><th>lower_bound</th><th>upper_bound</th></tr><tr><td>1</td><td>A</td><td>0.0</td><td>0.25</td></tr><tr><td>2</td><td>B</td><td>0.26</td><td>9.9</td></tr><tr><td>3</td><td>C</td><td>10.0</td><td>99.9</td></tr><tr><td>4</td><td>D</td><td>100.0</td><td>299.0</td></tr><tr><td>5</td><td>E</td><td>300.0</td><td>999.0</td></tr><tr><td>6</td><td>F</td><td>1000.0</td><td>4999.0</td></tr><tr><td>7</td><td>G</td><td>5000.0</td><td>null</td></tr></table>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 10\n",
    "fireSizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Owners dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ownersCols = List(owner_code, owner_descr)\n",
       "owners = [owner_id: bigint, owner_code: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[owner_id: bigint, owner_code: int ... 1 more field]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ownersCols = Seq(\n",
    "    \"owner_code\",\n",
    "    \"owner_descr\")\n",
    "\n",
    "val owners = createDimTable(stagedDF, \"owner_id\", ownersCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>owner_id</th><th>owner_code</th><th>owner_descr</th></tr><tr><td>1</td><td>0</td><td>FOREIGN</td></tr><tr><td>2</td><td>1</td><td>BLM</td></tr><tr><td>3</td><td>2</td><td>BIA</td></tr><tr><td>4</td><td>3</td><td>NPS</td></tr><tr><td>5</td><td>4</td><td>FWS</td></tr></table>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 5\n",
    "owners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locations dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "locationsCols = List(state, fips_name)\n",
       "locations = [location_id: bigint, state: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[location_id: bigint, state: string ... 1 more field]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val locationsCols = Seq(\n",
    "    \"state\",\n",
    "    \"fips_name\")\n",
    "\n",
    "val locations = createDimTable(stagedDF, \"location_id\", locationsCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>location_id</th><th>state</th><th>fips_name</th></tr><tr><td>1</td><td>AK</td><td>Prince of Wales-Outer Ketchikan</td></tr><tr><td>2</td><td>AK</td><td>Valdez-Cordova</td></tr><tr><td>3</td><td>AK</td><td>Wade Hampton</td></tr><tr><td>4</td><td>AK</td><td>Kenai Peninsula</td></tr><tr><td>5</td><td>AK</td><td>Ketchikan Gateway</td></tr></table>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 5\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createDatesDimDimension: (date_start: String, date_end: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def createDatesDimDimension(date_start:String, date_end:String): DataFrame = {\n",
    "    \n",
    "    def dayIterator(start: LocalDate, end: LocalDate) = Iterator.iterate(start)(_ plusDays 1) takeWhile (_ isBefore end)\n",
    "\n",
    "    val dates = dayIterator(LocalDate.parse(date_start), LocalDate.parse(date_end))\n",
    "    \n",
    "    val df_dates = spark.sparkContext\n",
    "        .parallelize(dates.map(_.toString).toSeq)\n",
    "        .toDF(\"date\")\n",
    "        .withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "        .coalesce(1)\n",
    "    \n",
    "    val date_dim = df_dates\n",
    "    .withColumn(\"date_dim_id\", monotonically_increasing_id() + 1)\n",
    "    .withColumn(\"date_type\", lit(\"date\"))\n",
    "    .withColumn(\"date_actual\", $\"date\")\n",
    "    .withColumn(\"epoch\", unix_timestamp($\"date\".cast(\"timestamp\")))\n",
    "    .withColumn(\"day_name\", date_format($\"date\", \"EEEE\"))\n",
    "    .withColumn(\"day_of_week\", dayofweek($\"date\"))\n",
    "    .withColumn(\"day_of_month\", dayofmonth($\"date\"))\n",
    "    .withColumn(\"day_of_year\", dayofyear($\"date\"))\n",
    "    .withColumn(\"week_of_month\", date_format($\"date\", \"W\"))\n",
    "    .withColumn(\"week_of_year\", weekofyear($\"date\"))\n",
    "    .withColumn(\"month_actual\", month($\"date\"))\n",
    "    .withColumn(\"month_name\", date_format($\"date\", \"MMMMM\"))\n",
    "    .withColumn(\"month_name_abbreviated\", date_format($\"date\", \"MMM\"))\n",
    "    .withColumn(\"quarter_actual\", quarter($\"date\"))\n",
    "    .withColumn(\"quarter_name\", \n",
    "                when($\"quarter_actual\" === 1, \"First\")\n",
    "                when($\"quarter_actual\" === 2, \"Second\")\n",
    "                when($\"quarter_actual\" === 3, \"Third\")\n",
    "                when($\"quarter_actual\" === 4, \"Fourth\"))\n",
    "    .withColumn(\"year_actual\", year($\"date\"))\n",
    "    .drop(\"date\")\n",
    "    \n",
    "    val max_id = date_dim.agg(\"date_dim_id\" -> \"max\").first().getLong(0) + 1\n",
    "    val unknownRecord = Seq((max_id, \"unknown\", null, null, null, null, null, null, null, null, null, null, null, null, null, null)).toDF()\n",
    "\n",
    "    val finalDatesDF = date_dim.union(unknownRecord)\n",
    "    finalDatesDF\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dates = [date_dim_id: bigint, date_type: string ... 14 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>date_dim_id</th><th>date_type</th><th>date_actual</th><th>epoch</th><th>day_name</th><th>day_of_week</th><th>day_of_month</th><th>day_of_year</th><th>week_of_month</th><th>week_of_year</th><th>month_actual</th><th>month_name</th><th>month_name_abbreviated</th><th>quarter_actual</th><th>quarter_name</th><th>year_actual</th></tr><tr><td>1</td><td>date</td><td>1990-01-01</td><td>631141200</td><td>Monday</td><td>2</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>January</td><td>Jan</td><td>1</td><td>First</td><td>1990</td></tr><tr><td>2</td><td>date</td><td>1990-01-02</td><td>631227600</td><td>Tuesday</td><td>3</td><td>2</td><td>2</td><td>1</td><td>1</td><td>1</td><td>January</td><td>Jan</td><td>1</td><td>First</td><td>1990</td></tr><tr><td>3</td><td>date</td><td>1990-01-03</td><td>631314000</td><td>Wednesday</td><td>4</td><td>3</td><td>3</td><td>1</td><td>1</td><td>1</td><td>January</td><td>Jan</td><td>1</td><td>First</td><td>1990</td></tr><tr><td>4</td><td>date</td><td>1990-01-04</td><td>631400400</td><td>Thursday</td><td>5</td><td>4</td><td>4</td><td>1</td><td>1</td><td>1</td><td>January</td><td>Jan</td><td>1</td><td>First</td><td>1990</td></tr><tr><td>5</td><td>date</td><td>1990-01-05</td><td>631486800</td><td>Friday</td><td>6</td><td>5</td><td>5</td><td>1</td><td>1</td><td>1</td><td>January</td><td>Jan</td><td>1</td><td>First</td><td>1990</td></tr></table>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 5\n",
    "val dates = createDatesDimDimension(\"1990-01-01\", \"2020-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather types dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weatherTypesCols = List(type)\n",
       "weatherTypes = [weather_type_id: bigint, type: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[weather_type_id: bigint, type: string]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weatherTypesCols = Seq(\n",
    "    \"type\")\n",
    "\n",
    "val weatherTypes = createDimTable(stagedWeatherDF, \"weather_type_id\", weatherTypesCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>weather_type_id</th><th>type</th></tr><tr><td>1</td><td>Strong Cold</td></tr><tr><td>2</td><td>Strong Hot</td></tr><tr><td>3</td><td>Weak Cold</td></tr><tr><td>4</td><td>Weak Hot</td></tr></table>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 5\n",
    "weatherTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facts table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During fact table design a couple of question arised:\n",
    "1. If we have date dimension, do we also need to have time dimension? If yes, it will be enourmous, if no, what to do with precise time querying?\n",
    "2. What to do with identifiers that can't be used as a fact table surrogate primary key but nevertheless need to be present in the datalake (e.g. FPA_ID)? \n",
    "\n",
    "If we look into Ralph Kimball's `The Data Warehouse ETL Toolkit` we can see that these question have been already answered.\n",
    "1. Creating a separate time dimension isn't recommended. We can use date dimension instead but if you want extra precision you can add full SQL datestamp directly to the fact table (`*_timestamp` columns in our case):\n",
    "\n",
    "> We may also want to compute very precise time intervals by comparing the exact time of two fact table records. For these reasons, we recommend the design shown in Figure 5.5. The calendar day component of the precise time remains as a foreign key reference to our familiar calendar day dimension. But we also embed a full SQL date-time stamp directly in the fact table for all queries requiring the extra precision. Think of this as special kind of fact, not a dimension. In this interesting case, it is not useful to make a dimension with the minutes or seconds component of the precise time stamp, because the calculation of time intervals across fact table records becomes too messy when trying to deal with separate day and time-of-day dimensions.\n",
    "\n",
    "2. This concept is known as a `degenerate dimension`. Some natural keys can be left as orphans during table design. It is recommended to leave in the fact table as pseudo-foreign key\n",
    "\n",
    "> We insert the original order number directly into the fact table as if it were a dimension key. We could have made a separate dimension out of this order number, but it would have turned out to contain only the order number, nothing else. For this reason, we give this natural key of the parent a special status and call it a degenerate (or empty) dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USA wildfires fact table\n",
    "Let's create a fact table using all prepared dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disc_dates = [disc_date_dim_id: bigint, disc_date_type: string ... 14 more fields]\n",
       "cont_dates = [cont_date_dim_id: bigint, cont_date_type: string ... 14 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[cont_date_dim_id: bigint, cont_date_type: string ... 14 more fields]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val disc_dates = dates.toDF(dates.columns.map(name => \"disc_\" + name):_*)\n",
    "val cont_dates = dates.toDF(dates.columns.map(name => \"cont_\" + name):_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stagedDFwithDates = [objectid: int, fod_id: int ... 34 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[objectid: int, fod_id: int ... 34 more fields]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stagedDFwithDates = stagedDF\n",
    "    .withColumn(\"disc_date_actual\", $\"discovery_timestamp\".cast(\"date\"))\n",
    "    .withColumn(\"cont_date_actual\", $\"cont_timestamp\".cast(\"date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a fact table using null-safe join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "firesFact = [fire_id: int, fpa_id: string ... 15 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>fire_id</th><th>fpa_id</th><th>discovery_date_id</th><th>cont_date_id</th><th>source_id</th><th>reporter_id</th><th>fire_name_id</th><th>fire_cause_id</th><th>fire_size_id</th><th>owner_id</th><th>location_id</th><th>fire_size</th><th>latitude</th><th>fire_year</th><th>longitude</th><th>discovery_timestamp</th><th>cont_timestamp</th></tr><tr><td>1869013</td><td>HIWMO-MA1857</td><td>6110</td><td>10958</td><td>1944</td><td>1502</td><td>76</td><td>13</td><td>6</td><td>15</td><td>543</td><td>2839.00</td><td>20.61781120</td><td>2006</td><td>-156.25547790</td><td>2006-09-23 00:00:00</td><td>null</td></tr><tr><td>1839763</td><td>SFO-2015NENFS20430</td><td>9193</td><td>9193</td><td>2615</td><td>1615</td><td>168</td><td>9</td><td>5</td><td>15</td><td>1667</td><td>500.00</td><td>41.98677000</td><td>2015</td><td>-99.86900000</td><td>2015-03-03 12:45:00</td><td>2015-03-03 20:00:00</td></tr><tr><td>1145328</td><td>AK-MSS-33452</td><td>7483</td><td>7483</td><td>1940</td><td>1123</td><td>502</td><td>4</td><td>1</td><td>8</td><td>16</td><td>0.10</td><td>61.60083390</td><td>2010</td><td>-149.23971560</td><td>2010-06-27 00:00:00</td><td>2010-06-27 04:57:00</td></tr><tr><td>1124091</td><td>CDF_2003_54_2220_007013      </td><td>5052</td><td>10958</td><td>2095</td><td>1477</td><td>852</td><td>2</td><td>1</td><td>15</td><td>191</td><td>0.10</td><td>40.43888888</td><td>2003</td><td>-121.85194444</td><td>2003-10-31 00:00:00</td><td>null</td></tr><tr><td>1125118</td><td>CDF_1995_54_2220_126         </td><td>2054</td><td>10958</td><td>2095</td><td>1477</td><td>873</td><td>9</td><td>3</td><td>15</td><td>191</td><td>10.00</td><td>40.32388888</td><td>1995</td><td>-122.43388888</td><td>1995-08-16 00:00:00</td><td>null</td></tr></table>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 5\n",
    "val firesFact = stagedDFwithDates\n",
    "    .join(broadcast(disc_dates), stagedDFwithDates(\"disc_date_actual\") <=> disc_dates(\"disc_date_actual\"), \"inner\")\n",
    "    .join(broadcast(cont_dates), stagedDFwithDates(\"cont_date_actual\") <=> cont_dates(\"cont_date_actual\"), \"inner\")\n",
    "    .join(broadcast(sources), \n",
    "          stagedDFwithDates(\"source_system_type\") <=> sources(\"source_system_type\") &&\n",
    "          stagedDFwithDates(\"source_system\") <=> sources(\"source_system\") &&\n",
    "          stagedDFwithDates(\"source_reporting_unit\") <=> sources(\"source_reporting_unit\") &&\n",
    "          stagedDFwithDates(\"source_reporting_unit_name\") <=> sources(\"source_reporting_unit_name\"),\n",
    "         \"inner\")\n",
    "    .join(broadcast(reports),    \n",
    "          stagedDFwithDates(\"nwcg_reporting_agency\") <=> reports(\"nwcg_reporting_agency\") &&\n",
    "          stagedDFwithDates(\"nwcg_reporting_unit_id\") <=> reports(\"nwcg_reporting_unit_id\") &&\n",
    "          stagedDFwithDates(\"nwcg_reporting_unit_name\") <=> reports(\"nwcg_reporting_unit_name\"),\n",
    "         \"inner\")\n",
    "    .join(fireNames,\n",
    "          stagedDFwithDates(\"fire_name\") <=> fireNames(\"fire_name\") &&\n",
    "          stagedDFwithDates(\"ics_209_name\") <=> fireNames(\"ics_209_name\") &&\n",
    "          stagedDFwithDates(\"fire_code\") <=> fireNames(\"fire_code\") &&\n",
    "          stagedDFwithDates(\"mtbs_fire_name\") <=> fireNames(\"mtbs_fire_name\"),\n",
    "         \"inner\")\n",
    "    .join(broadcast(fireCauses),\n",
    "          stagedDFwithDates(\"stat_cause_code\") <=> fireCauses(\"stat_cause_code\") &&\n",
    "          stagedDFwithDates(\"stat_cause_descr\") <=> fireCauses(\"stat_cause_descr\"),\n",
    "         \"inner\")\n",
    "    .join(broadcast(fireSizes),\n",
    "          stagedDFwithDates(\"fire_size_class\") <=> fireSizes(\"fire_size_class\"),\n",
    "          \"inner\"\n",
    "         )\n",
    "    .join(broadcast(owners),\n",
    "          stagedDFwithDates(\"owner_code\") <=> owners(\"owner_code\") &&\n",
    "          stagedDFwithDates(\"owner_descr\") <=> owners(\"owner_descr\"),\n",
    "          \"inner\"\n",
    "         )\n",
    "    .join(broadcast(locations),\n",
    "          stagedDFwithDates(\"state\") <=> locations(\"state\") &&\n",
    "          stagedDFwithDates(\"fips_name\") <=> locations(\"fips_name\"),  \n",
    "          \"inner\"\n",
    "         )\n",
    "    .select(\n",
    "        $\"objectid\" as \"fire_id\", \n",
    "        $\"fpa_id\",\n",
    "        $\"disc_date_dim_id\" as \"discovery_date_id\",\n",
    "        $\"cont_date_dim_id\" as \"cont_date_id\",\n",
    "        $\"source_id\",\n",
    "        $\"reporter_id\",\n",
    "        $\"fire_name_id\",\n",
    "        $\"fire_cause_id\",\n",
    "        $\"fire_size_id\",  \n",
    "        $\"owner_id\",\n",
    "        $\"location_id\",\n",
    "        $\"fire_size\",\n",
    "        $\"latitude\",\n",
    "        $\"fire_year\",\n",
    "        $\"longitude\",\n",
    "        $\"discovery_timestamp\".cast(\"string\"),\n",
    "        $\"cont_timestamp\".cast(\"string\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fire_id: integer (nullable = true)\n",
      " |-- fpa_id: string (nullable = true)\n",
      " |-- discovery_date_id: long (nullable = false)\n",
      " |-- cont_date_id: long (nullable = false)\n",
      " |-- source_id: long (nullable = false)\n",
      " |-- reporter_id: long (nullable = false)\n",
      " |-- fire_name_id: long (nullable = false)\n",
      " |-- fire_cause_id: long (nullable = false)\n",
      " |-- fire_size_id: long (nullable = false)\n",
      " |-- owner_id: long (nullable = false)\n",
      " |-- location_id: long (nullable = false)\n",
      " |-- fire_size: decimal(10,2) (nullable = true)\n",
      " |-- latitude: decimal(11,8) (nullable = true)\n",
      " |-- fire_year: integer (nullable = true)\n",
      " |-- longitude: decimal(11,8) (nullable = true)\n",
      " |-- discovery_timestamp: string (nullable = true)\n",
      " |-- cont_timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firesFact.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USA weather outliers fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_dates = [weather_date_dim_id: bigint, weather_date_type: string ... 14 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[weather_date_dim_id: bigint, weather_date_type: string ... 14 more fields]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather_dates = dates.toDF(dates.columns.map(name => \"weather_\" + name):_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weatherOutliersFactwithDates = [date_str: date, degrees_from_mean: string ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[date_str: date, degrees_from_mean: string ... 7 more fields]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weatherOutliersFactwithDates = stagedWeatherDF\n",
    "    .withColumn(\"weather_date_actual\", $\"date_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weatherOutliersFact = [weather_outlier_id: string, weather_date_dim_id: bigint ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>weather_outlier_id</th><th>weather_date_dim_id</th><th>weather_type_id</th><th>longitude</th><th>latitude</th><th>max_temp</th><th>min_temp</th></tr><tr><td>287175</td><td>6881</td><td>4</td><td>-99.0383</td><td>45.4478</td><td>20.0</td><td>0.0</td></tr><tr><td>300563</td><td>4553</td><td>4</td><td>-106.4278</td><td>36.2403</td><td>35.6</td><td>13.3</td></tr><tr><td>300564</td><td>4553</td><td>4</td><td>-84.9767</td><td>45.3725</td><td>31.1</td><td>20.6</td></tr><tr><td>300565</td><td>4553</td><td>2</td><td>-117.9528</td><td>36.1389</td><td>40.6</td><td>21.7</td></tr><tr><td>300566</td><td>4553</td><td>4</td><td>-110.395</td><td>40.1678</td><td>29.4</td><td>18.3</td></tr><tr><td>300567</td><td>4553</td><td>4</td><td>-95.2481</td><td>48.2325</td><td>25.6</td><td>21.1</td></tr><tr><td>300568</td><td>4553</td><td>4</td><td>-111.0125</td><td>39.2078</td><td>31.1</td><td>15.6</td></tr><tr><td>300569</td><td>4553</td><td>4</td><td>-106.3169</td><td>39.2292</td><td>26.1</td><td>0.0</td></tr><tr><td>300570</td><td>4553</td><td>4</td><td>-108.7339</td><td>39.1014</td><td>36.7</td><td>20.0</td></tr><tr><td>300571</td><td>4553</td><td>4</td><td>-100.9447</td><td>39.1131</td><td>40.0</td><td>16.7</td></tr></table>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 10\n",
    "val weatherOutliersFact = weatherOutliersFactwithDates\n",
    "    .join(broadcast(weather_dates), \n",
    "          weatherOutliersFactwithDates(\"weather_date_actual\") <=> weather_dates(\"weather_date_actual\"), \"inner\")\n",
    "    .join(broadcast(weatherTypes), \n",
    "          weatherOutliersFactwithDates(\"type\") <=> weatherTypes(\"type\"), \"inner\")\n",
    "    .select(\n",
    "        $\"serialid\" as \"weather_outlier_id\",\n",
    "        $\"weather_date_dim_id\",\n",
    "        $\"weather_type_id\",\n",
    "        $\"longitude\",\n",
    "        $\"latitude\",\n",
    "        $\"max_temp\",\n",
    "        $\"min_temp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Quality Checks\n",
    "Open `CapstoneDEND/athena/athena.ipynb` and run data quality checks using Amazon Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data dictionary \n",
    "Dimensions:\n",
    "* `sources` - tracks down a record to any original source that usa_wildfires was made from\n",
    "\n",
    "    * `source_system_type` - type of source database or system that the record was drawn from (federal, nonfederal, or interagency)\n",
    "    * `source_system` - name of or other identifier for source database or system that the record was drawn from\n",
    "    * `source_reporting_unit` - code for the agency unit preparing the fire report, based on code/name in the source dataset\n",
    "    * `ource_reporting_unit_name` - name of reporting agency unit preparing the fire report, based on code/name in the source dataset\n",
    "    \n",
    "    \n",
    "* `reporters` - lists agency names that reported a wild fire\n",
    "    * `nwcg_reporting_agency` - active National Wildlife Coordinating Group (NWCG) Unit Identifier for the agency preparing the fire report\n",
    "    * `nwcg_reporting_unit_id` - active NWCG Unit Identifier for the unit preparing the fire report\n",
    "    * `nwcg_reporting_unit_name` - active NWCG Unit Name for the unit preparing the fire report\n",
    "\n",
    "\n",
    "* `fire_names` - names of wild fires (if exist)\n",
    "    * `fire_name` - name of the incident, from the fire report (primary) or ICS-209 report (secondary)\n",
    "    * `ics_209_name` - name of the incident, from the ICS-209 report\n",
    "    * `fire_code` - code used within the interagency wildland fire community to track and compile cost information for emergency fire suppression\n",
    "    * `mtbs_fire_name` - name of the incident, from the MTBS perimeter dataset\n",
    "    \n",
    "\n",
    "* `fire_causes` - causes of fires and its codes\n",
    "    * `stat_cause_code` - code for the (statistical) cause of the fire\n",
    "    * `stat_cause_descr` - description of the (statistical) cause of the fire\n",
    "    \n",
    "\n",
    "* `fire_sizes` - codes for fire size based on the number of acres within the final fire perimeter expenditures\n",
    "    * `fire_size_class` - code for fire size based on the number of acres within the final fire perimeter expenditures \n",
    "    * `lower_bound` - lower bound of the amount of acres in a class\n",
    "    * `upper_bound` - upper bound of the amount of acres in a class\n",
    "    \n",
    "    \n",
    "* `locations` - locations of a wildfire\n",
    "    * `state` - state where a wildfire occured\n",
    "    * `fips_name` - county name\n",
    " \n",
    " \n",
    "* `owners` - codes for primary owners or entities responsible for managing the land at the point of origin of the fire\n",
    "    * `owner_code` - code for primary owner or entity responsible for managing the land at the point of origin of the fire at the time of the incident\n",
    "    * `owner_descr` - name of primary owner or entity responsible for managing the land at the point of origin of the fire at the time of the incident\n",
    "    \n",
    "\n",
    "* `weather_types` - codes for weather types\n",
    "    * `type` - type of variation as either 'weak cold', 'weak hot', 'strong hot', 'strong cold'\n",
    "    \n",
    "Fact tables:\n",
    "* `fires_fact` - main table with wildfires measurements\n",
    "    * `fire_id` - surrogate key of the table\n",
    "    * `fpa_id` - unique identifier that contains information necessary to track back to the original record in the source dataset \n",
    "    * `discovery_date_id` - look up column to `dates` table for wildfires discovery dates\n",
    "    * `cont_date_dim_id` - look up column to `dates` table for wildfires containment dates\n",
    "    * `source_id` - look up column to `sources` table\n",
    "    * `reporter_id` - look up column to `reportes` table\n",
    "    * `fire_name_id` - look up column to `fire_names` table\n",
    "    * `fire_size_id` - look up column to `fire_sizes` table\n",
    "    * `fire_cause_id` - look up column to `fire_causes` table\n",
    "    * `owner_id` - look up column to `owners` table\n",
    "    * `location_id` - look up column to `location` table\n",
    "    * `latitude` - latitude\n",
    "    * `longitude` - longitude\n",
    "    * `discovery_timestamp` - string with a date and time when a wildfires was discovered\n",
    "    * `cont_timestamp` - string with a date and time when a wildfires was contained\n",
    "\n",
    "\n",
    "* `weather_fact` - meain table with weather outliers measurements\n",
    "    * `weather_outlier_id` - surrogate key of the table\n",
    "    * `weather_date_dim_id` - look up column to `dates` table\n",
    "    * `weather_type_id` - look up column to `weather_types` table\n",
    "    * `longitude` - longitude\n",
    "    * `longitude` - longitude\n",
    "    * `max_temp` - daily high\n",
    "    * `min_temp` - daily low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Complete Project Write Up\n",
    "#### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "To clearly state the rationale for the choice of tools let's recap what technologies have been used throughout the project:\n",
    "* Apache Spark\n",
    "* AWS S3\n",
    "* AWS Athena\n",
    "\n",
    "\n",
    "1. Distributed vs non-distributed\n",
    "\n",
    "The rationale behind using distributed technologies (Apache Spark, AWS S3, AWS Athena) is dictated by the necessity to guarantee durability of data and long-term scaling potential. The volume of data used in this project doesn't exceed capacity of a single machine. It was possible to lay out star schema pattern in any local postgresql instance without the overhead of creating a scala spark job. But once established Big Data infrastructure allows to gracefully embrace increasing volumes of data without the need for remodelling data architecture and mitigating migration expenses. \n",
    "\n",
    "2. Amazon Athena vs Amazon Reshift as a querying arena\n",
    "\n",
    "Though data warehouse such as Amazon Redshift provides faster querying capabilities it's up to an end-user to decide whether one need a full-fledged Amazon Redshift cluster that demands upscaling every time amount of your data exceeds limits or you can put up with less perfomant queries for the ability to pay as you go for 5$ per 1 TB of data scanned by Amazon Athena service. My conclusion is that sooner or later your data warehouse will have to store too much data to handle it gracefully and the need of SQL querying on data stored in S3 will arise: either in the form Amazon Redshift Spectrum or Amazon Athena. it is reasonable to start using Amazon Athena right from the start without overpaying for Redshift given the fact that Presto SQL engine supports pretty much every piece of SQL functionality that is needed for a data analyst to provide insights. A small bonus is that unlike Amazon Redshift SQL Presto SQL supports GROUPING SETS and CUBE functions which are really handy for making data marts via Amazon Athena views for Tableau reporting.\n",
    "\n",
    "#### Propose how often the data should be updated and why\n",
    "\n",
    "I would recommnd a source system to generate its output on demand. Data should be generated and processed when a wildfire occurs.\n",
    "\n",
    "#### Write a description of how you would approach the problem differently under the following scenarios:\n",
    "Questions:\n",
    "1. The data was increased by 100x.\n",
    "2. The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "3. The database needed to be accessed by 100+ people.\n",
    "\n",
    "Answers:\n",
    "1. When choosing building a datalake on S3 with Apache Spark as a processing engine and Amazon Athena as a querying arena we had already laid a foundation for gracefully handling 100x increase in data. If reasonably partitioned and stored in columnar data storage format like parquet or ORC 100x increase will not be noticed.\n",
    "\n",
    "2. For daily incremental uploads it would be reasonable to use any workflow management system like Apache Airflow with its EmrCreateJobFlowOperator for running spark jobs\n",
    "\n",
    "3. Amazon Athena provides easy access to 100+ people unlike Amazon Redshift where you will need to finetune workload management query queues. If you still need a warehouse with the ability to organically accept hundreds of analysts Vertica might be something you can look at."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
